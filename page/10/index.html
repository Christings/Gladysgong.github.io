<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"gongyanli.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Up in the wind!">
<meta property="og:type" content="website">
<meta property="og:title" content="茉莉Python">
<meta property="og:url" content="http://gongyanli.com/page/10/index.html">
<meta property="og:site_name" content="茉莉Python">
<meta property="og:description" content="Up in the wind!">
<meta property="og:locale">
<meta property="article:author" content="Lilly">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://gongyanli.com/page/10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>茉莉Python</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">茉莉Python</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">voidqueens@hotmail.com</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://gongyanli.com/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lilly">
      <meta itemprop="description" content="Up in the wind!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="茉莉Python">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/" class="post-title-link" itemprop="url">数据库基本操作</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-10 13:26:07" itemprop="dateCreated datePublished" datetime="2018-04-10T13:26:07+08:00">2018-04-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-18 21:34:40" itemprop="dateModified" datetime="2020-09-18T21:34:40+08:00">2020-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="一-增加"><a href="#一-增加" class="headerlink" title="一.增加"></a>一.增加</h3><pre><code>使用insert插入单行数据
insert [into] &lt;表名&gt; [列名] values &lt;列值&gt;
insert into Students (name,sex,birth_date) values (&#39;李三&#39;,&#39;男&#39;,&#39;1990/6/15&#39;)
insert into Students values (&#39;李四&#39;,&#39;男&#39;,&#39;1990/6/15&#39;)

使用insert,select语句将现有表中的数据添加到已有的新表中
insert into &lt;已有的新表&gt; &lt;列名&gt; select &lt;原表列名&gt; from &lt;原表名&gt;
insert into addressList (&#39;姓名&#39;,&#39;地址&#39;,&#39;电子邮件&#39;) select name,address,email from Students</code></pre>
<h3 id="二-删除–使用delete删除数据库中某些数据"><a href="#二-删除–使用delete删除数据库中某些数据" class="headerlink" title="二.删除–使用delete删除数据库中某些数据"></a>二.删除–使用delete删除数据库中某些数据</h3><pre><code>delete from &lt;表名&gt; [where &lt;删除条件&gt;]
delete from where name=&#39;李三&#39;  -- 删除整行而不是某个单一字段</code></pre>
<h3 id="三-改–使用update更新数据库中数据"><a href="#三-改–使用update更新数据库中数据" class="headerlink" title="三.改–使用update更新数据库中数据"></a>三.改–使用update更新数据库中数据</h3><pre><code>update &lt;表名&gt; set &lt;列名=更新值&gt; [where &lt;更新条件&gt;]
update Students set tel=default where id=5
update students set age=age+1
update students set name=&quot;张伟鹏&quot;, age=19 where tel=&quot;13288097888&quot;</code></pre>
<h3 id="四-查"><a href="#四-查" class="headerlink" title="四.查"></a>四.查</h3><h4 id="4-1-普通查询"><a href="#4-1-普通查询" class="headerlink" title="4.1 普通查询"></a>4.1 普通查询</h4><pre><code>select &lt;列名&gt; from &lt;表名&gt; [where &lt;查询条件表达试&gt;] [order by&lt;排序的列名&gt;[asc或desc]]
1.查询所有数据行和列
select * from Students
2.查询部分行列--条件查询
select * from Student where name=&#39;李三&#39;
3.在查询中使用as更改列名
select name as 姓名 from Students where sex=&#39;男&#39;
4.查询空行
select name from Student where sex is null
5.查询排序
select name from Student where age&gt;25 order by desc
6.查询返回限制行数
select top 10 name from Student</code></pre>
<h4 id="4-2-模糊查询"><a href="#4-2-模糊查询" class="headerlink" title="4.2 模糊查询"></a>4.2 模糊查询</h4><pre><code>1.使用like进行模糊查询
select * from Student where name like &#39;李%&#39;
2.使用between在某个范围内进行查询
select * from Student where age between 20 and 25
3.使用in在列举数值内进行查询(in后是多个的数据)
select name from Student where address in (&#39;北京&#39;，&#39;上海&#39;,&#39;深圳&#39;)</code></pre>
<h4 id="4-3-分组查询"><a href="#4-3-分组查询" class="headerlink" title="4.3 分组查询"></a>4.3 分组查询</h4><pre><code>1.使用group by进行分组查询
select id as 学号，AVG(score) as 平均成绩 from score group by id
2.用having子句进行分组筛选
select id as 学号，AVG(score) as 平均成绩 from score group by id having count(score)&gt;1</code></pre>
<h4 id="4-4-多表内联接查询"><a href="#4-4-多表内联接查询" class="headerlink" title="4.4 多表内联接查询"></a>4.4 多表内联接查询</h4><pre><code>select Students.name,score.mark from Students,score where Students.name=score.name</code></pre>
<h3 id="五-数据库表的连接"><a href="#五-数据库表的连接" class="headerlink" title="五.数据库表的连接"></a>五.数据库表的连接</h3><pre><code>左连：Left Join
右连：Right Join
内连：Inner Join
左连接where只影响右表，右连接where只影响左表

select * from table1 Left Join table2 where table1.id=table2.id
左连接后的检索结果是显示tbl1的所有数据和tbl2中满足where 条件的数据。

select * from table1 Right Join table2 where table1.id=table2.id
右连接后的检索结果是tbl2的所有数据和tbl1中满足where 条件的数据。

select * from table1 Inner Join table2 on table1.id=table2.id
功能和 select * from tbl1,tbl2 where tbl1.id=tbl2.id相同。</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://gongyanli.com/%E6%95%B0%E6%8D%AE%E5%BA%93-1-%E5%9F%BA%E7%A1%80%E7%AF%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lilly">
      <meta itemprop="description" content="Up in the wind!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="茉莉Python">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93-1-%E5%9F%BA%E7%A1%80%E7%AF%87/" class="post-title-link" itemprop="url">数据库基本操作</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-10 13:26:07" itemprop="dateCreated datePublished" datetime="2018-04-10T13:26:07+08:00">2018-04-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-18 21:39:42" itemprop="dateModified" datetime="2020-09-18T21:39:42+08:00">2020-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="一-增加"><a href="#一-增加" class="headerlink" title="一.增加"></a>一.增加</h3><pre><code>使用insert插入单行数据
insert [into] &lt;表名&gt; [列名] values &lt;列值&gt;
insert into Students (name,sex,birth_date) values (&#39;李三&#39;,&#39;男&#39;,&#39;1990/6/15&#39;)
insert into Students values (&#39;李四&#39;,&#39;男&#39;,&#39;1990/6/15&#39;)

使用insert,select语句将现有表中的数据添加到已有的新表中
insert into &lt;已有的新表&gt; &lt;列名&gt; select &lt;原表列名&gt; from &lt;原表名&gt;
insert into addressList (&#39;姓名&#39;,&#39;地址&#39;,&#39;电子邮件&#39;) select name,address,email from Students</code></pre>
<h3 id="二-删除–使用delete删除数据库中某些数据"><a href="#二-删除–使用delete删除数据库中某些数据" class="headerlink" title="二.删除–使用delete删除数据库中某些数据"></a>二.删除–使用delete删除数据库中某些数据</h3><pre><code>delete from &lt;表名&gt; [where &lt;删除条件&gt;]
delete from where name=&#39;李三&#39;  -- 删除整行而不是某个单一字段</code></pre>
<h3 id="三-改–使用update更新数据库中数据"><a href="#三-改–使用update更新数据库中数据" class="headerlink" title="三.改–使用update更新数据库中数据"></a>三.改–使用update更新数据库中数据</h3><pre><code>update &lt;表名&gt; set &lt;列名=更新值&gt; [where &lt;更新条件&gt;]
update Students set tel=default where id=5
update students set age=age+1
update students set name=&quot;张伟鹏&quot;, age=19 where tel=&quot;13288097888&quot;</code></pre>
<h3 id="四-查"><a href="#四-查" class="headerlink" title="四.查"></a>四.查</h3><h4 id="4-1-普通查询"><a href="#4-1-普通查询" class="headerlink" title="4.1 普通查询"></a>4.1 普通查询</h4><pre><code>select &lt;列名&gt; from &lt;表名&gt; [where &lt;查询条件表达试&gt;] [order by&lt;排序的列名&gt;[asc或desc]]
1.查询所有数据行和列
select * from Students
2.查询部分行列--条件查询
select * from Student where name=&#39;李三&#39;
3.在查询中使用as更改列名
select name as 姓名 from Students where sex=&#39;男&#39;
4.查询空行
select name from Student where sex is null
5.查询排序
select name from Student where age&gt;25 order by desc
6.查询返回限制行数
select top 10 name from Student</code></pre>
<h4 id="4-2-模糊查询"><a href="#4-2-模糊查询" class="headerlink" title="4.2 模糊查询"></a>4.2 模糊查询</h4><pre><code>1.使用like进行模糊查询
select * from Student where name like &#39;李%&#39;
2.使用between在某个范围内进行查询
select * from Student where age between 20 and 25
3.使用in在列举数值内进行查询(in后是多个的数据)
select name from Student where address in (&#39;北京&#39;，&#39;上海&#39;,&#39;深圳&#39;)</code></pre>
<h4 id="4-3-分组查询"><a href="#4-3-分组查询" class="headerlink" title="4.3 分组查询"></a>4.3 分组查询</h4><pre><code>1.使用group by进行分组查询
select id as 学号，AVG(score) as 平均成绩 from score group by id
2.用having子句进行分组筛选
select id as 学号，AVG(score) as 平均成绩 from score group by id having count(score)&gt;1</code></pre>
<h4 id="4-4-多表内联接查询"><a href="#4-4-多表内联接查询" class="headerlink" title="4.4 多表内联接查询"></a>4.4 多表内联接查询</h4><pre><code>select Students.name,score.mark from Students,score where Students.name=score.name</code></pre>
<h3 id="五-数据库表的连接"><a href="#五-数据库表的连接" class="headerlink" title="五.数据库表的连接"></a>五.数据库表的连接</h3><pre><code>左连：Left Join
右连：Right Join
内连：Inner Join
左连接where只影响右表，右连接where只影响左表

select * from table1 Left Join table2 where table1.id=table2.id
左连接后的检索结果是显示tbl1的所有数据和tbl2中满足where 条件的数据。

select * from table1 Right Join table2 where table1.id=table2.id
右连接后的检索结果是tbl2的所有数据和tbl1中满足where 条件的数据。

select * from table1 Inner Join table2 on table1.id=table2.id
功能和 select * from tbl1,tbl2 where tbl1.id=tbl2.id相同。</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://gongyanli.com/%E6%95%B0%E6%8D%AE%E5%BA%93-2-%E4%B8%AD%E7%BA%A7%E7%AF%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lilly">
      <meta itemprop="description" content="Up in the wind!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="茉莉Python">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93-2-%E4%B8%AD%E7%BA%A7%E7%AF%87/" class="post-title-link" itemprop="url">数据库基本操作</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-10 13:26:07" itemprop="dateCreated datePublished" datetime="2018-04-10T13:26:07+08:00">2018-04-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-18 21:35:41" itemprop="dateModified" datetime="2020-09-18T21:35:41+08:00">2020-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="三、数据库"><a href="#三、数据库" class="headerlink" title="三、数据库"></a>三、数据库</h2><p>1、数据库引擎</p>
<pre><code>种类：
    innob引擎 
    Memory引擎 
    Merge引擎</code></pre>
<p>innob引擎一大特点就是支持外键，内存和空间大 支持事务。</p>
<p>2、数据库锁</p>
<p>由于数据库是多用户共享资源，所以需要处理并发问题，而数据库锁的机制就是为了处理这一问题。</p>
<p>当出现并发的时候，如果不做控制就会出现各种问题 比如脏数据，修改丢失等。</p>
<p>所有数据库并发需要事务来控制，事务并发问题需要数据库锁来控制</p>
<p>事务四个特性：</p>
<pre><code>持久型 
原子性 
一致性 
隔离性</code></pre>
<p>数据库锁：</p>
<pre><code>乐观锁 悲观锁 死锁

活锁 行锁 表锁 页级锁

排它锁有称为写锁。共享锁又称为读锁</code></pre>
<p>3、如何理解ORM？</p>
<p>对象关系映射（Object Relational Mapping，简称ORM）是通过使用描述对象和数据库之间映射的元数据，将面向对象语言程序中的对象自动持久化到关系数据库中。本质上就是将数据从一种形式转换到另外一种形式。</p>
<p><img src="https://gypsy-1255824480.cos.ap-beijing.myqcloud.com/blog/orm.png" alt="https://gypsy-1255824480.cos.ap-beijing.myqcloud.com/blog/orm.png"></p>
<p>作用:</p>
<pre><code>节省代码书写 保证sql语法的正确性
一次编写可以适配多个数据库
防止sql注入
在数据库表名或字段名发生改变 只需要修改模型类的映射 无需修改数据库操作的代码</code></pre>
<p>使用orm的方式选择:</p>
<pre><code>1.先创建模型类 在迁移到数据库中

优点：简单快捷，定义一次模型类即可，不用写sql
缺点： 
不能完美的控制创建表的所有细节问题
表结构发生变化时，也难免发生迁移错误

2.先用原生sql创建数据库表，在编写模型类做映射

优点：可以很好的控制数据库表结构的任何细节，避免发生迁移错误
缺点：可能编写工作多</code></pre>
<p>4、ORM相关操作<br>5、selected_related和prefetch_related<br>6、Q和F<br>7、queryset常用方法</p>
<pre><code>使用 connection.queries 可以查看sql语句
filter 将满足条件的结果返回，返回值为QuerySet对象
exclude 将满足条件的结果过滤掉,返回值为QuerySet对象
annotate 给QuerySet中的每一个对象都添加一个查询表达式，（Q,F,聚合函数）的字段
order_by 安装某个字段进行排序，默认为从小到大排序，如果想要重大到小可以在字段前加“-”,需要注意order_by可以传递多个参数，会按照先后级别进行排序，而且order_by还可以用annotate新增的字段来排序，注意：order_by重复调用会只会保存最后一个。
values:提取需要哪些字段，默认会把全部都提取出来。返回的结果是QuerySet，但是其中包裹的不是模型，而是字典，如果想要提取关联数据，那么可以通过F表达式来完成。values也可以使用聚合函数
values_list 提取需要字段，返回QuerySet，其中包裹的元组，如果数据只有一条，则可以使用flat=True进行扁平化处理，直接返回结果
all 方法：返回全部数据（返回包裹对象的QuerySet）
select_related : 可以将关联的对象一起查询出来，只能用于外键连接的形式（一对多或者一对一的形式）
prefetch_related : 使用的时候传入反向引用的时候调用的名称默认为 filed_set，使用这个方式可以用来查询多对一或者多对多的方式，也可以用来查询一对一或者一对多的，但是不推荐。
defer : 过滤掉不需要的字段，返回值是一个包裹着模型的QuerySet
only : 提取某些字段，返回一个包裹模型的QuerySet
get : 直接返回对象，只能返回一条数据，如果返回数据超过一条就会报错，如果没有匹配到任何数据也会报错。
create : 用于增加一条数据，并且将数据保存在数据库中，相当于先创建数据，然后调用save方法
get_or_create : 如果给定的条件存在数据就查找返回，如果不存在就创建一个，然后在返回，返回结果是一个元组，元组中两个内容，第一个是模型对象，第二个是bool值，如果没有创建返回False，如果创建了返还True。
bulk_create ：可以一次性创建多条数据。
count ：获取满足条件的数据的个数
first,last 分别返回QuerySet中第一条和最后一条数据
exists ： 判断数据是否存在，存在返回True，不存在返回False
distinct : 去重，默认按照全部字段判断。注意如果和order_by同时使用会失效
update ：更新数据
delete : 删除数据，需要注意：字段中on_delete的级联删除方式</code></pre>
<p>8、数据库常用操作：</p>
<p>user（用户）,group（部门）,role（角色）三个表</p>
<pre><code>查询年龄大于18的人
select * from user where age &gt; 18

查询年龄不等于18的人
select * from user where age !=18

查询 IT部 和 运维部的所有人
select * from user where group = &#39;IT部&#39; and group =&#39;运维部&#39;

查询 IT部 或 运维部的所有人
select * from user where group = &#39;IT部&#39; or group =&#39;运维部&#39;

查询角色是 “管理员” 的所有人
select * from user where role = &#39;管理员&#39;

设计数据库：会议室预定

设计数据库：员工、部门、角色

查询 “IT”部门所有人
select * from user where group=&#39;IT&#39;

查询 每个部门 的员工数量
select count(*) from user where group?

查询 每个部门 年龄不等于 18的人

查询部门人数不满 5 的部门？</code></pre>
<p>9、数据库优化方案</p>
<pre><code>尽量避免使用select * 能用字段名就用字段名，避免查询无用字段

select count(*)会查全表，尽量避免

建表时字段类型能用varchar/nvarchar就不要用char/nchar

避免频繁的创建和删除临时表，会耗费性能资源，产生大量log

如果使用临时表，在使用的最后一定要显示删除，先trancate table，再drop table

尽量避免大事务操作，提高并发效率

避免向客户端返回大数据量，数据量过大，应考虑需求是否合理

比如你在一个在线网站使用delete和update操作，必然会引发数据库锁</code></pre>
<p>10、数据库索引以及注意点？<br>11、什么情况下建索引？<br>12、数据库索引种类？<br>13、drop、delete和truncate区别</p>
<pre><code>drop直接删掉表；

truncate删除的是表中的数据，再插入数据时自增长的数据id又重新从1开始；

delete删除表中数据，可以在后面添加where字句。</code></pre>
<p>14、数据库中出现乱码？如何解决？<br>15、执行计划和慢日志？</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/bluecobra/archive/2012/01/11/2318922.html">https://www.cnblogs.com/bluecobra/archive/2012/01/11/2318922.html</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/ZhangQiye1993/article/details/80717606">https://blog.csdn.net/ZhangQiye1993/article/details/80717606</a><br>16、数据库读写分离</p>
<p>参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/h18208975507/article/details/99458396">https://blog.csdn.net/h18208975507/article/details/99458396</a></p>
<p>主从同步使得数据可以从一个数据库服务器复制到其他服务器上，在复制数据时，一个服务器充当主服务器（master），其余的服务器充当从服务器（slave）。因为复制是异步进行的，所以从服务器不需要一直连接着主服务器，从服务器甚至可以通过拨号断断续续地连接主服务器。通过配置文件，可以指定复制所有的数据库，某个数据库，甚至是某个数据库上的某个表。</p>
<p>主从同步的好处：</p>
<pre><code>通过增加从服务器来提高数据库的性能，在主服务器上执行写入和更新，在从服务器上向外提供读功能，可以动态地调整从服务器的数量，从而调整整个数据库的性能。

提高数据安全，因为数据已复制到从服务器，从服务器可以终止复制进程，所以，可以在从服务器上备份而不破坏主服务器相应数据

在主服务器上生成实时数据，而在从服务器上分析这些数据，从而提高主服务器的性能</code></pre>
<p>读写分离的基本原理就是让主数据库处理事务性增,改,删操作(INSERT,UPDATE,DELETE)操作,而从数据库处理SELECT查询操作,数据库复制被用来把事物性操作导致的变更同步到其他从数据库,以SQL为例,主数据库负责写数据,读数据,读库仅负责读数据,每次有写库操作,同步更新到读库,写库就一个,读库可以有多个,采用日志同步的方式实现主库和多个数据库的数据同步</p>
<p>具体配置如下：<br>1）在配置文件中增加slave数据库的配置</p>
<p>在Django的配置文件settings.py中,DATABASES中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">DATABASES &#x3D; &#123;</span><br><span class="line">    &#39;default&#39;: &#123;</span><br><span class="line">        &#39;ENGINE&#39;: &#39;django.db.backends.mysql&#39;,</span><br><span class="line">        &#39;HOST&#39;: &#39;127.0.0.1&#39;,  # 主服务器的运行ip</span><br><span class="line">        &#39;PORT&#39;: 3306,   # 主服务器的运行port</span><br><span class="line">        &#39;USER&#39;: &#39;django&#39;,  # 主服务器的用户名</span><br><span class="line">        &#39;PASSWORD&#39;: &#39;django&#39;,  # 主服务器的密码</span><br><span class="line">        &#39;NAME&#39;: &#39;djangobase&#39;   #  数据表名</span><br><span class="line">    &#125;,</span><br><span class="line">    &#39;slave&#39;: &#123;</span><br><span class="line">        &#39;ENGINE&#39;: &#39;django.db.backends.mysql&#39;, </span><br><span class="line">        &#39;HOST&#39;: &#39;127.0.0.1&#39;,</span><br><span class="line">        &#39;PORT&#39;: 8306,</span><br><span class="line">        &#39;USER&#39;: &#39;django_slave&#39;,</span><br><span class="line">        &#39;PASSWORD&#39;: &#39;django_slave&#39;,</span><br><span class="line">        &#39;NAME&#39;: &#39;djangobase_slave&#39;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;　　</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>创建数据库操作的路由分类</li>
</ol>
<p>在项目的utils中创建db_router.py文件,并在该文件中定义一个db类,用来进行读写分离</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class MasterSlaveDBRouter(object):</span><br><span class="line">    &quot;&quot;&quot;数据库主从读写分离路由&quot;&quot;&quot;</span><br><span class="line"> </span><br><span class="line">    def db_for_read(self, model, **hints):</span><br><span class="line">        &quot;&quot;&quot;读数据库&quot;&quot;&quot;</span><br><span class="line">        return &quot;slave&quot;</span><br><span class="line"> </span><br><span class="line">    def db_for_write(self, model, **hints):</span><br><span class="line">        &quot;&quot;&quot;写数据库&quot;&quot;&quot;</span><br><span class="line">        return &quot;default&quot;</span><br><span class="line"> </span><br><span class="line">    def allow_relation(self, obj1, obj2, **hints):</span><br><span class="line">        &quot;&quot;&quot;是否运行关联操作&quot;&quot;&quot;</span><br><span class="line">        return True　</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>配置读写分离路由</li>
</ol>
<p>在配置文件setting.py中增加:</p>
<p>#配置读写分离<br>    DATABASE_ROUTERS = [‘项目名.utils.db_router.MasterSlaveDBRouter’]</p>
<p>17、用过什么ORM框架？</p>
<p>使用Python进行MySQL的库主要有三个：<br>Python-MySQL（更熟悉的名字可能是MySQLdb），<br>PyMySQL<br>SQLAlchemy。</p>
<p>Python-MySQL：<br>资格最老，核心由C语言打造，接口精炼，性能最棒，缺点是环境依赖较多，安装复杂，近两年已停止更新，<br>只支持Python2，不支持Python3</p>
<p>PyMySQL：<br>为替代Python-MySQL而生，纯python打造，<br>接口与Python-MySQL兼容，安装方便，支持Python3</p>
<p>SQLAlchemy:<br>是一个ORM框架，它并不提供底层的数据库操作，<br>而是要借助于MySQLdb、PyMySQL等第三方库来完成，目前SQLAlchemy在Web编程领域应用广泛。</p>
<p>orm种类：Django’s ORM、peewee、SQLAlchemy</p>
<p>Django’s ORM</p>
<pre><code>优点：
    易用，学习曲线短
    和Django紧密集合，用Django时使用约定俗成的方法去操作数据库

缺点：
    QuerySet速度不给力，会逼我用Mysqldb来操作原生sql语句。
    以前写过一篇关于django orm 跟mysqldb性能的对比</code></pre>
<p>Peewee</p>
<pre><code>优点：
    Django式的API，使其易用
    轻量实现，很容易和任意web框架集成

缺点：
    不支持自动化 schema 迁移
    不能像Django那样，使线上的mysql表结构生成结构化的模型。</code></pre>
<p>SQLAlchemy</p>
<pre><code>优点：
    巨牛逼的API，使得代码有健壮性和适应性
    灵活的设计，使得能轻松写复杂查询

缺点：
    工作单元概念不常见
 重量级 API，导致长学习曲线</code></pre>
<p>18、ORM缺点和优点？</p>
<p>ORM（对象映射关系程序）：通过orm将编程语言的对象模型和数据库的关系模型建立映射关系，这样我们在使用编程语言对数据库进行操作的时候可以直接使用编程语言的对象模型进行操作就可以了，而不用直接使用sql语言。</p>
<p>orm的优点：</p>
<p>隐藏了数据访问细节，“封闭”的通用数据库交互，ORM的核心。他使得我们的通用数据库交互变得简单易行，并且完全不用考虑该死的SQL语句。快速开发，由此而来。<br>ORM使我们构造固化数据结构变得简单易行。</p>
<p>缺点：</p>
<p>无可避免的，自动化意味着映射和关联管理，代价是牺牲性能（早期，这是所有不喜欢ORM人的共同点）。现在的各种ORM框架都在尝试使用各种方法来减轻这块（LazyLoad，Cache），效果还是很显著的。</p>
<p>ORM 的优点：</p>
<pre><code>• 数据模型都在一个地方定义，更容易更新和维护，也利于重用代码。
• ORM 有现成的工具，很多功能都可以自动完成，比如数据消毒、预处理、事务等等。
• 它迫使你使用 MVC 架构，ORM 就是天然的 Model，最终使代码更清晰。
• 基于 ORM 的业务代码比较简单，代码量少，语义性好，容易理解。
• 你不必编写性能不佳的 SQL。</code></pre>
<p>ORM 的缺点：</p>
<pre><code>• ORM 库不是轻量级工具，需要花很多精力学习和设置。
• 对于复杂的查询，ORM 要么是无法表达，要么是性能不如原生的 SQL。
• ORM 抽象掉了数据库层，开发者无法了解底层的数据库操作，也无法定制一些特殊的 SQL。</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://gongyanli.com/%E5%B8%82%E5%9C%BA%E4%B8%8A%E7%88%AC%E8%99%AB%E4%BA%A7%E5%93%81%E5%88%86%E6%9E%90%E5%9B%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lilly">
      <meta itemprop="description" content="Up in the wind!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="茉莉Python">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E5%B8%82%E5%9C%BA%E4%B8%8A%E7%88%AC%E8%99%AB%E4%BA%A7%E5%93%81%E5%88%86%E6%9E%90%E5%9B%BE/" class="post-title-link" itemprop="url">市场上爬虫产品调研分析图</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-08 18:37:30" itemprop="dateCreated datePublished" datetime="2018-04-08T18:37:30+08:00">2018-04-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-18 17:15:43" itemprop="dateModified" datetime="2020-09-18T17:15:43+08:00">2020-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%92%8C%E6%8A%93%E5%8F%96/" itemprop="url" rel="index"><span itemprop="name">数据分析和抓取</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="http://p2lakvkq0.bkt.clouddn.com/%E7%88%AC%E8%99%AB%E4%BA%A7%E5%93%81%E5%88%86%E6%9E%90.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://gongyanli.com/Flask+Scrapy+MongoDB%E5%BC%80%E5%8F%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lilly">
      <meta itemprop="description" content="Up in the wind!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="茉莉Python">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Flask+Scrapy+MongoDB%E5%BC%80%E5%8F%91/" class="post-title-link" itemprop="url">Flask+Scrapy+MongoDB开发</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-04 11:01:32" itemprop="dateCreated datePublished" datetime="2018-04-04T11:01:32+08:00">2018-04-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-18 17:14:11" itemprop="dateModified" datetime="2020-09-18T17:14:11+08:00">2020-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%92%8C%E6%8A%93%E5%8F%96/" itemprop="url" rel="index"><span itemprop="name">数据分析和抓取</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%92%8C%E6%8A%93%E5%8F%96/Scrapy/" itemprop="url" rel="index"><span itemprop="name">Scrapy</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>代码传送门：<a target="_blank" rel="noopener" href="https://github.com/Gladysgong/flask_scrapy_aiot">https://github.com/Gladysgong/flask_scrapy_aiot</a>  </p>
</blockquote>
<p>前言：最近心血来潮，想把以前做的事情总结一下，这个小demo是2017年8月左右写的把，很幼稚也有很多需要改进的地方。如果看了的朋友觉得有<br>帮助的话，github上动动小手指给我个star把，我现在很需要star，你懂得啦，多谢多谢可爱的各位捧场。</p>
<h2 id="一、总体思路"><a href="#一、总体思路" class="headerlink" title="一、总体思路"></a>一、总体思路</h2><pre><code>1.利用Scrapy抓取一些农业网站的信息，存储进入MongoDB数据库；
2.利用Flask搭建我们的后端；
3.读取MongoDB中存储的信息，并展示在前端；</code></pre>
<p><img src="http://p2lakvkq0.bkt.clouddn.com/flask.jpg">   </p>
<pre><code>目录结构如下，分为flask_aiot和scrapy_aiot两块，其中flask——aiot中包含前后端代码，而scrapy_aiot中包含爬虫代码。很简单把，
现在让我们开启这趟Flask+Scrapy+MongoDB之旅把！</code></pre>
<h2 id="二、scrapy-aiot爬虫模块"><a href="#二、scrapy-aiot爬虫模块" class="headerlink" title="二、scrapy-aiot爬虫模块"></a>二、scrapy-aiot爬虫模块</h2><p>在这个模块中，我抓取了5个网站的信息存储进入MongoDB数据库，数据只要是文章和价格，下面具体来看把。</p>
<h3 id="1-items定义"><a href="#1-items定义" class="headerlink" title="1.items定义"></a>1.items定义</h3><pre><code># chinacwa——中国智慧农业网
class ChinacwaItem(scrapy.Item):
    # 文章标题、关键字、图片地址、摘要、内容地址、内容
    article_id = scrapy.Field()
    article_title = scrapy.Field()
    article_keywords = scrapy.Field()
    article_imageurl = scrapy.Field()
    article_abstract = scrapy.Field()
    article_url = scrapy.Field()
    article_content = scrapy.Field()

# iot——国家农业物联网
class IotItem(scrapy.Item):
    article_id = scrapy.Field()
    article_title = scrapy.Field()
    article_keywords = scrapy.Field()
    article_abstract = scrapy.Field()
    article_url = scrapy.Field()
    article_content = scrapy.Field()

# ny135——中国农业物联网
class Ny135Item(scrapy.Item):
    article_id = scrapy.Field()
    article_title = scrapy.Field()
    article_keywords = scrapy.Field()
    article_abstract = scrapy.Field()
    article_url = scrapy.Field()
    article_content = scrapy.Field()

# productprice——农产品价格
class ProductpriceItem(scrapy.Item):
    product_name = scrapy.Field()
    product_lowestprice = scrapy.Field()
    product_averageprice = scrapy.Field()
    product_highestprice = scrapy.Field()
    product_specification = scrapy.Field()
    product_unit = scrapy.Field()
    product_releasedate = scrapy.Field()

# AProductsPrice——全国农产品价格
class AllProductsPriceItem(scrapy.Item):
    product_name = scrapy.Field()
    product_price = scrapy.Field()
    product_market = scrapy.Field()
    product_releasedate = scrapy.Field()</code></pre>
<h3 id="2-爬虫文件"><a href="#2-爬虫文件" class="headerlink" title="2.爬虫文件"></a>2.爬虫文件</h3><p>主要就是发起请求，然后解析response中的内容。对于这几个网站我都是用的CrawlSpider，因为可以做到整站抓取，很方便。来看看iot.py<br>文件中的内容把。其余四个文件就不一一展示了，可以去github下载来看。<a target="_blank" rel="noopener" href="https://github.com/Gladysgong/flask_scrapy_aiot">传送门</a></p>
<pre><code># 国家农业物联网
class IotSpider(CrawlSpider):
    name = &quot;IotSpider&quot;
    allowed_domains = [&#39;iot-cn.org&#39;]
    start_urls = [&#39;http://www.iot-cn.org&#39;]
    rules = [
        Rule(LinkExtractor(allow=(&#39;/news/show.php&#39;)),
             callback=&#39;parse_article&#39;,
             follow=True)
    ]

    def parse_article(self, response):
        item = IotItem()
        sel = Selector(response)
        article_title = sel.xpath(&#39;//div[@class=&quot;m&quot;]/div[1]/div[@class=&quot;left_box&quot;]/h1[@id=&quot;title&quot;]/text()&#39;).extract()[0]
        article_keywords = sel.xpath(
            &#39;//div[@class=&quot;m&quot;]/div[1]/div[@class=&quot;left_box&quot;]/div[@class=&quot;keytags&quot;]/a/text()&#39;).extract()
        article_abstract = sel.xpath(
            &#39;//div[@class=&quot;m&quot;]/div[1]/div[@class=&quot;left_box&quot;]/div[@class=&quot;introduce&quot;]/text()&#39;).extract()[0]
        article_url = response.url
        article_content = sel.xpath(
            &#39;//div[@class=&quot;m&quot;]/div[1]/div[@class=&quot;left_box&quot;]/div[@id=&quot;content&quot;]/div&#39;).xpath(&#39;string(.)&#39;).extract()[0]

        item[&#39;article_title&#39;] = article_title
        item[&#39;article_keywords&#39;] = article_keywords
        item[&#39;article_abstract&#39;] = article_abstract
        item[&#39;article_url&#39;] = article_url
        item[&#39;article_content&#39;] = article_content

        print(&quot;article_title:&quot;, article_title)
        print(&quot;article_keywords:&quot;, article_keywords)
        print(&quot;article_abstract:&quot;, article_abstract)
        print(&quot;article_url:&quot;, article_url)
        print(&quot;article_content:&quot;, article_content)

        yield item</code></pre>
<h3 id="3-数据持久化–MongoDB"><a href="#3-数据持久化–MongoDB" class="headerlink" title="3.数据持久化–MongoDB"></a>3.数据持久化–MongoDB</h3><pre><code># 农业物联网
class AiotPipeline(object):
    def __init__(self):
        self.client = pymongo.MongoClient(host=settings[&#39;MONGO_HOST&#39;], port=settings[&#39;MONGO_PORT&#39;])
        self.db = self.client[settings[&#39;MONGO_DB&#39;]]
        # self.coll = self.db[settings[&#39;MONGO_COLL2&#39;]]
        self.chinacwa = self.db[&#39;chinacwa&#39;]
        self.iot = self.db[&#39;iot&#39;]
        self.ny135 = self.db[&#39;ny135&#39;]
        self.productprice = self.db[&#39;productprice&#39;]
        self.allproductprice = self.db[&#39;allproductprice&#39;]

    def process_item(self, item, spider):
        if isinstance(item, ChinacwaItem):
            try:
                if item[&#39;article_title&#39;]:
                    item = dict(item)
                    self.chinacwa.insert(item)
                    print(&quot;插入成功&quot;)
                    return item
            except Exception as e:
                spider.logger.exceptionn(&quot;&quot;)
        elif isinstance(item, IotItem):
            try:
                if item[&#39;article_title&#39;]:
                    item = dict(item)
                    self.iot.insert(item)
                    print(&quot;成功&quot;)
                    return item
            except Exception as e:
                spider.logger.exceptionn(&quot;&quot;)
        elif isinstance(item, Ny135Item):
            try:
                if item[&#39;article_title&#39;]:
                    item = dict(item)
                    self.ny135.insert(item)
                    print(&quot;插入&quot;)
                    return item
            except Exception as e:
                spider.logger.exceptionn(&quot;ny135存储失败&quot;)
        elif isinstance(item, ProductpriceItem):
            try:
                if item[&quot;product_name&quot;]:
                    item = dict(item)
                    self.productprice.insert(item)
                    print(&quot;农产品价格数据&quot;)
                    return item
            except Exception as e:
                spider.logger.exceptionn(&quot;农产品价格数据存储失败&quot;)
        elif isinstance(item, AllProductsPriceItem):
            try:
                if item[&quot;product_name&quot;]:
                    item = dict(item)
                    self.allproductprice.insert(item)
                    print(&quot;全国农产品价格数据&quot;)
                    return item
            except Exception as e:
                spider.logger.exceptionn(&quot;全国农产品价格数据存储失败&quot;)</code></pre>
<h2 id="三、flask-aiot模块"><a href="#三、flask-aiot模块" class="headerlink" title="三、flask_aiot模块"></a>三、flask_aiot模块</h2><pre><code>功能点:登录，注册，数据展示，搜索
熟悉Flask的朋友可以直接看懂，不熟悉Flask的话就得需要看点儿东西呢，写到这里打算抽时间写一篇如何使用Flask来创建一个web网站的
的文章，有了这样子的基础后，就能很轻易往下看了。</code></pre>
<h3 id="1-定义model"><a href="#1-定义model" class="headerlink" title="1.定义model"></a>1.定义model</h3><pre><code>class Chinacwa(mongo.Document):
    article_id = mongo.IntField(required=True)
    article_title = mongo.StringField(required=True)
    article_keywords = mongo.StringField(required=True)
    article_url = mongo.StringField(required=True)
    article_abstract = mongo.StringField(required=True)
    article_content = mongo.StringField(required=True)

    meta = &#123;
        &#39;collection&#39;: &#39;chinacwa&#39;,
        &#39;ordering&#39;: [&#39;-article_id&#39;],
        &#39;indexes&#39;: [&#39;-article_id&#39;]
    &#125;

class Iot(mongo.Document):
    article_title = mongo.StringField(required=True)
    article_keywords = mongo.StringField(required=True)
    article_url = mongo.StringField(required=True)
    article_abstract = mongo.StringField(required=True)
    article_content = mongo.StringField(required=True)

    meta = &#123;&#39;collection&#39;: &#39;iot&#39;&#125;

class Ny135(mongo.Document):
    article_title = mongo.StringField(required=True)
    article_keywords = mongo.StringField(required=True)
    article_url = mongo.StringField(required=True)
    article_abstract = mongo.StringField(required=True)
    article_content = mongo.StringField(required=True)
    meta = &#123;&#39;collection&#39;: &#39;ny135&#39;&#125;

class AllProductPrice(mongo.Document):
    product_name = mongo.StringField(required=True)
    product_price = mongo.StringField(required=True)
    product_market = mongo.StringField(required=True)
    product_releasedate = mongo.StringField(required=True)
    meta = &#123;&#39;collection&#39;: &#39;allproductprice&#39;&#125;


class User(UserMixin, mongo.Document):
    # uid = mongo.IntField(requires=True)
    email = mongo.StringField(max_length=255, requires=True)
    username = mongo.StringField(max_length=255, requires=True)
    # password = mongo.StringField(requires=True)
    password_hash = mongo.StringField(requires=True)
    # confirmed=mongo.BooleanField(default=False)
    # password_hash = mongo.StringField(requires=True)
    # meta = &#123;&#39;collection&#39;: &#39;user&#39;&#125;

    @property
    def password(self):
        raise AttributeError(&#39;password is not a readable attribute&#39;)

    @password.setter
    def password(self, password):
        self.password_hash = generate_password_hash(password)

    def verify_password(self, password):
        return check_password_hash(self.password_hash, password)

    def __repr__(self):
        return &#39;&lt;User %r&gt;&#39; % self.email

    def get_id(self):
        try:
            # return unicode(self.username)
            return self.email
        except AttributeError:
            raise NotImplementedError(&#39;No `username` attribute - override `get_id`&#39;)

    def __unicode__(self):
        return self.email

        # def confirm(self,token):
        #     s=Serializer(current_app.config[&#39;SECRET_KEY&#39;])
        #     try:
        #         data=s.loads(token)
        #     except:
        #         return False
        #     if data.get(&#39;confirm&#39;)!=self.username
        #         return False
        #     self.confirmd=True

@login_manager.user_loader
def load_user(email):
    try:
        user = User.objects.get(email=email)
    except User.DoesNotExist:
        user = None
    return user</code></pre>
<h3 id="2-待续未完"><a href="#2-待续未完" class="headerlink" title="2.待续未完"></a>2.待续未完</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://gongyanli.com/%E5%88%A9%E7%94%A8Scrapy%E4%B8%8B%E8%BD%BD%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8Cexcel%E6%95%B0%E6%8D%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lilly">
      <meta itemprop="description" content="Up in the wind!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="茉莉Python">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E5%88%A9%E7%94%A8Scrapy%E4%B8%8B%E8%BD%BD%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8Cexcel%E6%95%B0%E6%8D%AE/" class="post-title-link" itemprop="url">利用Scrapy下载世界银行excel文件</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-03 16:36:53" itemprop="dateCreated datePublished" datetime="2018-04-03T16:36:53+08:00">2018-04-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-18 17:14:47" itemprop="dateModified" datetime="2020-09-18T17:14:47+08:00">2020-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%92%8C%E6%8A%93%E5%8F%96/" itemprop="url" rel="index"><span itemprop="name">数据分析和抓取</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%92%8C%E6%8A%93%E5%8F%96/Scrapy/" itemprop="url" rel="index"><span itemprop="name">Scrapy</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>本文首发于我的博客：<a href="http://gongyanli.com/%E5%88%A9%E7%94%A8Scrapy%E4%B8%8B%E8%BD%BD%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8Cexcel%E6%95%B0%E6%8D%AE/">http://gongyanli.com</a><br>代码传送门：<a target="_blank" rel="noopener" href="https://github.com/Gladysgong/wordbank">https://github.com/Gladysgong/wordbank</a><br>简书: <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/b8253ad8054e">https://www.jianshu.com/p/b8253ad8054e</a><br>CSDN: <a target="_blank" rel="noopener" href="https://blog.csdn.net/u012052168/article/details/79806493">https://blog.csdn.net/u012052168/article/details/79806493</a></p>
</blockquote>
<h2 id="一、总体思路"><a href="#一、总体思路" class="headerlink" title="一、总体思路"></a>一、总体思路</h2><pre><code>    我的目标是下载世界银行中各个指标的excel文件，刚好世界银行给我们提供了excel下载页面的url地址，这样子我们只需要构建url地址进行
请求就好了，还蛮简单的，也不会太大劲。
    首先我需要把所有指标的地址拿到，于是我找到了这个地址**https://data.worldbank.org/indicator?tab=all**，通过这个地址拿到所
有指标的href，再进行拼接，最后把拼接的结果进行请求。</code></pre>
<p><img src="http://p2lakvkq0.bkt.clouddn.com/worldbank.jpg"></p>
<h2 id="二、item模块"><a href="#二、item模块" class="headerlink" title="二、item模块"></a>二、item模块</h2><pre><code>class WorldBankItem(scrapy.Item):
    indi_url = scrapy.Field()  # 指标(indicator)的url
    indi_name = scrapy.Field()  # 指标(indicator)的名字</code></pre>
<h2 id="二、爬虫模块"><a href="#二、爬虫模块" class="headerlink" title="二、爬虫模块"></a>二、爬虫模块</h2><pre><code>1.解析url
    def parse_urls(self, response):
        item = WorldBankItem()
        selector = scrapy.Selector(response)

        indicators = selector.xpath(&#39;//*[@id=&quot;main&quot;]/div[2]&#39;)
        indi_url = indicators.xpath(&#39;section[@class=&quot;nav-item&quot;]/ul/li/a/@href&#39;).extract()
        # indi = re.findall(r&#39;/indicator/.*/?view=chart&#39;, indicators, re.S)
        indi_name = indicators.xpath(&#39;section[@class=&quot;nav-item&quot;]/ul/li/a/text()&#39;).extract()

        for each in indi_url:
            each = each[:-10] + &quot;downloadformat=excel&quot;
            # i.replace(&quot;view=chart&quot;, &quot;downloadformat=excel&quot;) #使用replace进行替换时总是不成功，有待探索！
            item[&#39;indi_url&#39;] = each
            print(&quot;indi_url&quot;, item[&#39;indi_url&#39;])
            yield item
            yield scrapy.Request(url=&quot;http://api.worldbank.org/v2/en&quot; + each,
                                 callback=self.download_excel)

        for each in indi_name:
            print(&quot;indi_name:&quot;, each)
            item[&#39;indi_name&#39;] = each
            # self.filenames = indi_name
            yield item
2.下载excel文件并写入
    def download_excel(self, response):
        name_temp = response.url.split(&quot;/&quot;)[-1]
        name = name_temp.split(&quot;?&quot;)[-2]
        print(&quot;storename:&quot;, name, &#39;-&#39;, response.url)
        filename = r&quot;D:\workspace\scrapy\worldbank\worldbankexcelfiles\%s.xls&quot; % name
        resp = requests.get(response.url)
        output = open(filename, &#39;wb&#39;)
        output.write(resp.content)
        output.close()
        return None</code></pre>
<h2 id="四、数据持久化"><a href="#四、数据持久化" class="headerlink" title="四、数据持久化"></a>四、数据持久化</h2><pre><code>1.定义mysql类（属于我单独定义的）
    import pymysql
    class Mysql:
        def __init__(self, host, user, pwd, db):
            self.host = host
            self.user = user
            self.pwd = pwd
            self.db = db

        def __GetConnect(self):
            if not self.db:
                raise (NameError, &#39;数据库不存在&#39;)
            self.conn = pymysql.connect(host=self.host, user=self.user, password=self.pwd, database=self.db, charset=&#39;utf8&#39;)
            cur = self.conn.cursor()
            if not cur:
                raise (NameError, &#39;账号或密码错误&#39;)
            else:
                return cur

        def ExecQuery(self, sql):
            cur = self.__GetConnect()
            cur.execute(sql)
            resList = cur.fetchall()

            self.conn.close()
            return resList

        def ExecNoQuery(self, sql):
            cur = self.__GetConnect()
            cur.execute(sql)
            self.conn.commit()
            self.conn.close()
2.pipelines
    from worldbank.db.mysql import Mysql
    from worldbank.items import WorldBankItem


    class WorldbankPipeline(object):
        def process_item(self, item, spider):
            if isinstance(item, WorldBankItem):
                mysql = Mysql(host=&#39;localhost&#39;, user=&#39;root&#39;, pwd=&#39;421498&#39;, db=&#39;saas&#39;)
                if len(item[&#39;indi_name&#39;]) == 0:
                    pass
                else:
                    newsql = &quot;insert into worldbank_indi(indi_url,indi_name)values(&#39;%s&#39;,&#39;%s&#39;)&quot; % (
                        item[&#39;indi_url&#39;], item[&#39;indi_name&#39;])
                    print(newsql)
                    mysql.ExecNoQuery(newsql.encode(&#39;utf-8&#39;))
            else:
                pass
            return item</code></pre>
<h2 id="五、设置settings"><a href="#五、设置settings" class="headerlink" title="五、设置settings"></a>五、设置settings</h2><pre><code>ITEM_PIPELINES = &#123;
       &#39;worldbank.pipelines.WorldbankPipeline&#39;: 300,&#125; # 记得开启此处</code></pre>
<h2 id="六、bug"><a href="#六、bug" class="headerlink" title="六、bug"></a>六、bug</h2><pre><code>    在爬虫模块parse_urls()中我不光拼接了url地址，我还把指标的url和name放进了item中，因为我这边考虑的，excel文件命名的时候我是
用的url的一部分命名的，像这样子**EN.ATM.GHGO.KT.CE**，这是属于指标名字的简写，的确我们手工下载数据的时候也是以这个命名。但是像我
这种对指标不熟悉的人，完全看不出简写的含义，所以我就想把简写以及指标的全名存储进入数据库，以方便对照，所以我用的yield item这样子来
返回数据。
    但是实际存储的时候，总是报错**KeyError: &#39;indi_name&#39;**，但是数据也确实存进了数据库，所以我很不理解，这个问题有待于探索，也希
望知道的朋友可以告知。
    本来我也试过用Request中meta来传递item，然后一起返回，但是插入数据库的时候，报错主键的值必须唯一。
    有可能和用的数据库也有关系，用MySQL的数据来存储爬虫数据很不顺手，因为需要自己手工建立数据库和表，或者写代码建立。而MongoDB就很
方便了，告诉数据库名字和表名，自动就帮我们创建了。</code></pre>
<h2 id="七、我是二傻"><a href="#七、我是二傻" class="headerlink" title="七、我是二傻"></a>七、我是二傻</h2><pre><code>    原来上面的问题我早就解决了，只是我忘记了，果然好记性不如烂笔头。
parse_url()换成如下：
其实是把两个for改成了一个for，但是这样子就需要把list换成str来进行存储，并且存储的时候我遇到了转义字符的问题，报错如下：
**pymysql.err.ProgrammingError: (1064, &#39;You have an error in your SQL syntax; check the manual that corresponds to 
your MySQL server version for the right syntax to use near \&#39;Recipe&quot; of Machine Learning&quot;,&quot;https://i.ytimg.com/vi/
DkgJ_VkU5jM/hqdefault.jpg&quot;,\&#39; at line 4&#39;)**
改pipelines文件，把字段包裹上pymysql.escape_string(),同时我已经将代码更新，可以自己去看。

    def parse_urls(self, response):
        item = WorldBankItem()
        selector = scrapy.Selector(response)

        indicators = selector.xpath(&#39;//*[@id=&quot;main&quot;]/div[2]/section[@class=&quot;nav-item&quot;]/ul/li&#39;)

        for i in indicators:
            temp_url = i.xpath(&#39;a/@href&#39;).extract()  # 得到的结果为list
            # indi_url = str(temp_url)[:-12] + &quot;downloadformat=excel&quot;
            indi_url = str(temp_url).replace(&quot;view=chart&quot;, &quot;downloadformat=excel&quot;)
            item[&quot;indi_url&quot;] = indi_url.replace(&quot;&#39;&quot;, &quot;&quot;).replace(&quot;[&quot;, &quot;&quot;).replace(&quot;]&quot;, &#39;&#39;)
            # print(&#39;item[&quot;indi_url&quot;]:&#39;, item[&quot;indi_url&quot;])

            indi_name = i.xpath(&#39;a/text()&#39;).extract()
            item[&quot;indi_name&quot;] = str(indi_name)
            # print(&#39;item[&quot;indi_name&quot;]:&#39;, item[&quot;indi_name&quot;])
            yield item

            url = indi_url.replace(&quot;&#39;&quot;, &quot;&quot;).replace(&quot;[&quot;, &quot;&quot;).replace(&quot;]&quot;, &#39;&#39;)
            yield scrapy.Request(url=&quot;http://api.worldbank.org/v2/en&quot; + url, callback=self.download_excel)</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://gongyanli.com/%E5%9F%BA%E4%BA%8E%E5%85%B3%E9%94%AE%E5%AD%97%E5%9C%A8%E4%B8%BB%E6%B5%81%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E4%B8%AD%E6%8A%93%E5%8F%96%E4%BF%A1%E6%81%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lilly">
      <meta itemprop="description" content="Up in the wind!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="茉莉Python">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E5%9F%BA%E4%BA%8E%E5%85%B3%E9%94%AE%E5%AD%97%E5%9C%A8%E4%B8%BB%E6%B5%81%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E4%B8%AD%E6%8A%93%E5%8F%96%E4%BF%A1%E6%81%AF/" class="post-title-link" itemprop="url">基于关键字在主流搜索引擎中抓取信息</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-30 19:54:52" itemprop="dateCreated datePublished" datetime="2018-03-30T19:54:52+08:00">2018-03-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-18 17:15:13" itemprop="dateModified" datetime="2020-09-18T17:15:13+08:00">2020-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%92%8C%E6%8A%93%E5%8F%96/" itemprop="url" rel="index"><span itemprop="name">数据分析和抓取</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%92%8C%E6%8A%93%E5%8F%96/Scrapy/" itemprop="url" rel="index"><span itemprop="name">Scrapy</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>本文首发于我的博客：<a href="http://gongyanli.com/%E5%9F%BA%E4%BA%8E%E5%85%B3%E9%94%AE%E5%AD%97%E5%9C%A8%E4%B8%BB%E6%B5%81%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E4%B8%AD%E6%8A%93%E5%8F%96%E4%BF%A1%E6%81%AF/">http://gongyanli.com</a><br>代码传送门：<a target="_blank" rel="noopener" href="https://github.com/Gladysgong/seCrawler">https://github.com/Gladysgong/seCrawler</a><br>简书: <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/4e244563849a">https://www.jianshu.com/p/4e244563849a</a><br>CSDN: <a target="_blank" rel="noopener" href="https://blog.csdn.net/u012052168/article/details/79762586">https://blog.csdn.net/u012052168/article/details/79762586</a></p>
</blockquote>
<h1 id="seCrawler-Search-Engine-Crawler"><a href="#seCrawler-Search-Engine-Crawler" class="headerlink" title="seCrawler(Search Engine Crawler)"></a>seCrawler(Search Engine Crawler)</h1><p>A scrapy project can crawl search result of Google/Bing/Baidu<br>基于scrapy来做的爬虫项目，可以根据关键字来抓取从百度、bing、google中所搜索到的结果</p>
<h2 id="1-refer"><a href="#1-refer" class="headerlink" title="1.refer"></a>1.refer</h2><p>copying by <a target="_blank" rel="noopener" href="https://github.com/xtt129/seCrawler">https://github.com/xtt129/seCrawler</a> and rewrite,adding title and abstract.</p>
<h2 id="prerequisite"><a href="#prerequisite" class="headerlink" title="prerequisite"></a>prerequisite</h2><p>python 3.5 and scrapy is needed.</p>
<h2 id="commands"><a href="#commands" class="headerlink" title="commands"></a>commands</h2><p>run one command to get 50 pages result from search engine with keyword, the result would be kept in the “urls.txt” under the current directory.</p>
<p>####Bing<br><code>scrapy crawl keywordSpider -a keyword=Spider-Man -a se=bing -a pages=50</code></p>
<p>####Baidu<br><code>scrapy crawl keywordSpider -a keyword=Spider-Man -a se=baidu -a pages=50</code></p>
<p>####Google<br><code>scrapy crawl keywordSpider -a keyword=Spider-Man -a se=google -a pages=50</code></p>
<h2 id="results"><a href="#results" class="headerlink" title="results"></a>results</h2><p>url,title and abstract will be stored in the urls.txt</p>
<h2 id="limitation"><a href="#limitation" class="headerlink" title="limitation"></a>limitation</h2><p>The project doesn’t provide any workaround to the anti-spider measure like CAPTCHA, IP ban list, etc. </p>
<p>But to reduce these measures, we recommand to set <code>DOWNLOAD_DELAY=10</code> in settings.py file to add a temporisation (in second) between the crawl of two pages, see details in <a target="_blank" rel="noopener" href="https://doc.scrapy.org/en/1.2/topics/settings.html#std:setting-DOWNLOAD_DELAY">Scrapy Setting</a>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://gongyanli.com/%E5%88%A9%E7%94%A8Scrapy%E6%8A%93%E5%8F%96%E4%B8%80%E5%B8%A6%E4%B8%80%E8%B7%AF%E6%88%98%E7%95%A5%E6%80%A7%E6%94%AF%E6%92%91%E5%B9%B3%E5%8F%B0%E4%BF%A1%E6%81%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lilly">
      <meta itemprop="description" content="Up in the wind!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="茉莉Python">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E5%88%A9%E7%94%A8Scrapy%E6%8A%93%E5%8F%96%E4%B8%80%E5%B8%A6%E4%B8%80%E8%B7%AF%E6%88%98%E7%95%A5%E6%80%A7%E6%94%AF%E6%92%91%E5%B9%B3%E5%8F%B0%E4%BF%A1%E6%81%AF/" class="post-title-link" itemprop="url">利用Scrapy抓取一带一路战略性支撑平台信息</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-30 18:31:31" itemprop="dateCreated datePublished" datetime="2018-03-30T18:31:31+08:00">2018-03-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-18 17:14:56" itemprop="dateModified" datetime="2020-09-18T17:14:56+08:00">2020-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%92%8C%E6%8A%93%E5%8F%96/" itemprop="url" rel="index"><span itemprop="name">数据分析和抓取</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%92%8C%E6%8A%93%E5%8F%96/Scrapy/" itemprop="url" rel="index"><span itemprop="name">Scrapy</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>本文首发于我的博客：<a href="http://gongyanli.com/%E5%88%A9%E7%94%A8Scrapy%E6%8A%93%E5%8F%96%E4%B8%80%E5%B8%A6%E4%B8%80%E8%B7%AF%E6%88%98%E7%95%A5%E6%80%A7%E6%94%AF%E6%92%91%E5%B9%B3%E5%8F%B0%E4%BF%A1%E6%81%AF/">http://gongyanli.com</a><br>代码传送门：<a target="_blank" rel="noopener" href="https://github.com/Gladysgong/BeltandRoad">https://github.com/Gladysgong/BeltandRoad</a><br>简书: <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/6fe4afa1b98a">https://www.jianshu.com/p/6fe4afa1b98a</a><br>CSDN: <a target="_blank" rel="noopener" href="https://blog.csdn.net/u012052168/article/details/79761833">https://blog.csdn.net/u012052168/article/details/79761833</a></p>
</blockquote>
<pre><code>好久以前做的东西了，网站的数据也很容易拿到，最近想对自己的东西做一个总结，所以就有了这篇文章。    
我的目标是抓取一带一路战略支撑平台里面一些机构的数据，像联系电话、邮箱等等，简单看看网站的样子把。
首先获取列表项的所有url，其中包括一项翻页操作，拿到url后访问，获取里面的详细信息，就这么简单。</code></pre>
<p><img src="http://p2lakvkq0.bkt.clouddn.com/beltandroad.jpg"></p>
<h2 id="一、定义Items"><a href="#一、定义Items" class="headerlink" title="一、定义Items"></a>一、定义Items</h2><pre><code>`class BeltandRoadItem(scrapy.Item):
# collection = &#39;BeltandRoad&#39;

name = scrapy.Field()  # 机构名称
intro = scrapy.Field()  # 机构简介
address = scrapy.Field()  # 机构地址
tel = scrapy.Field()  # 机构电话
fax = scrapy.Field()  # 机构传真
email = scrapy.Field()  # 机构邮箱
site = scrapy.Field()  # 机构网址`</code></pre>
<h2 id="二、爬虫模块"><a href="#二、爬虫模块" class="headerlink" title="二、爬虫模块"></a>二、爬虫模块</h2><pre><code>爬虫模块中包括翻页操作，并不复杂，就不详细说明。</code></pre>
<hr>
<pre><code>`# -*- coding:utf-8 -*-
from scrapy.spiders import CrawlSpider, Rule
import scrapy
from ..items import BeltandRoadItem

# from scrapy.conf import settings

# 一带一路战略支撑平台
class BeltandRoadSpider(CrawlSpider):
    name = &quot;BeltandRoadSpider&quot;
    start_urls = [&#39;http://ydyl.drcnet.com.cn/www/ydyl/channel.aspx?version=YDYL&amp;uid=8011&#39;]

    # 解析url地址
    def parse(self, response):
        # urls = response.xpath(&#39;//div[@class=&quot;pub_right&quot;]/ul/li/div[1]/a/@href&#39;).extract()
        urls = response.xpath(&#39;//ul[@id=&quot;ContentPlaceHolder1_WebPageDocumentsByUId1&quot;]/li/div[1]/a/@href&#39;).extract()
        # institute_name = response.xpath(&#39;//ul[@class=&quot;left-nav&quot;]/li/h3/a/text()&#39;).extract()

        for url in urls:
            # url = &quot;http://ydyl.drcnet.com.cn/www/ydyl/&quot; + url
            # print(&quot;1:&quot;, url)
            yield scrapy.Request(url=url, callback=self.parse_content)

        next_page = response.xpath(
            &#39;//div[@id=&quot;ContentPlaceHolder1_WebPageDocumentsByUId1_PageRow&quot;]/input[4]/@onclick&#39;).extract()
        next_page = str(next_page).split(&quot;\&#39;&quot;)[1]
        print(&quot;next:&quot;, next_page)
        if next_page:
            next_page = &quot;http://ydyl.drcnet.com.cn&quot; + next_page
            yield scrapy.Request(url=next_page, callback=self.parse)

    # 解析内容
    def parse_content(self, response):
        item = BeltandRoadItem()
        name = response.xpath(&#39;//div[@id=&quot;disArea&quot;]/strong/div/text()&#39;).extract()[0]  # 提取名称
        content = response.xpath(&#39;//div[@id=&quot;disArea&quot;]/div[@id=&quot;docContent&quot;]/p&#39;).xpath(&#39;string(.)&#39;).extract()  # 提取其他信息
        content = str(content).split(&#39;\&#39;&#39;)
        item[&#39;name&#39;] = name
        for i in range(len(content)):  # 过滤无效信息后，提取有用信息存储到item中
            if content[i] != &#39;[&#39; and content[i] != &#39;]&#39; and content[i] != &#39;, &#39;:
                print(&quot;xx:&quot;, content[i])
                if &quot;简介&quot; in content[i]:
                    item[&#39;intro&#39;] = content[i]
                elif &quot;地址&quot; in content[i]:
                    item[&#39;address&#39;] = content[i]
                elif &quot;联系电话&quot; in content[i]:
                    item[&#39;tel&#39;] = content[i]
                elif &quot;传真&quot; in content[i]:
                    item[&#39;fax&#39;] = content[i]
                elif &quot;电子邮箱&quot; in content[i]:
                    item[&#39;email&#39;] = content[i]
                elif &quot;网址&quot; in content[i]:
                    item[&#39;site&#39;] = content[i]
        yield item
`</code></pre>
<h2 id="三、构建pipelines"><a href="#三、构建pipelines" class="headerlink" title="三、构建pipelines"></a>三、构建pipelines</h2><pre><code>主要就是进行数据持久化操作，把数据存入MongoDB数据中。</code></pre>
<hr>
<pre><code>`import pymongo
from scrapy.conf import settings
from .items import BeltandRoadItem

# 一带一路战略支撑平台
class BeltandRoadPipeline(object):
    def __init__(self, mongo_uri, mongo_db, mongo_port):
        self.mongo_uri = mongo_uri
        self.mongo_port = mongo_port
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        return cls(mongo_uri=crawler.settings.get(&#39;MONGO_URI&#39;),
                   mongo_port=crawler.settings.get(&#39;MONGO_PORT&#39;),
                   mongo_db=crawler.settings.get(&#39;MONGO_DB&#39;)
                   )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri, self.mongo_port)
        self.db = self.client[self.mongo_db]
        self.BeltandRoad = self.db[&#39;BeltandRoad&#39;]

    def close_spider(self, spider):
        self.client.close()

    def process_item(self, item, spider):
        if isinstance(item, BeltandRoadItem):
            try:
                if item[&#39;name&#39;]:
                    item = dict(item)
                    # self.db[item.collection].insert(item)  运行这个代码时利用items中collection创建表，会提示插入失败，但是依然会插入到数据库？
                    self.BeltandRoad.insert(item)
                    print(&quot;插入成功&quot;)
                    return item
            except Exception as e:
                spider.logger.exception(&quot;插入失败&quot;)`</code></pre>
<h2 id="四、配置文件settings"><a href="#四、配置文件settings" class="headerlink" title="四、配置文件settings"></a>四、配置文件settings</h2><pre><code># 激活pipelines
ITEM_PIPELINES = &#123;
   &#39;BeltandRoad.pipelines.BeltandRoadPipeline&#39;: 300,&#125;

# 数据库配置
MONGO_URI = &quot;127.0.0.1&quot;  # 主机IP
MONGO_PORT = 27017  # 端口号
MONGO_DB = &quot;Belt&quot;  # 数据库名字</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://gongyanli.com/PhantomJS+Selenium+Scrapy%E6%8A%93%E5%8F%96%E5%B7%A8%E6%BD%AE%E8%B5%84%E8%AE%AF%E7%BD%91%E4%BC%81%E4%B8%9A%E4%BF%A1%E6%81%AF(%E4%B8%80)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lilly">
      <meta itemprop="description" content="Up in the wind!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="茉莉Python">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/PhantomJS+Selenium+Scrapy%E6%8A%93%E5%8F%96%E5%B7%A8%E6%BD%AE%E8%B5%84%E8%AE%AF%E7%BD%91%E4%BC%81%E4%B8%9A%E4%BF%A1%E6%81%AF(%E4%B8%80)/" class="post-title-link" itemprop="url">PhantomJS+Selenium+Scrapy抓取巨潮资讯网企业信息(一)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-30 10:39:34" itemprop="dateCreated datePublished" datetime="2018-03-30T10:39:34+08:00">2018-03-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-18 17:27:32" itemprop="dateModified" datetime="2020-09-18T17:27:32+08:00">2020-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%92%8C%E6%8A%93%E5%8F%96/" itemprop="url" rel="index"><span itemprop="name">数据分析和抓取</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%92%8C%E6%8A%93%E5%8F%96/Scrapy/" itemprop="url" rel="index"><span itemprop="name">Scrapy</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>本文首发于我的博客：<a href="http://gongyanli.com/PhantomJS-Selenium-Scrapy%E6%8A%93%E5%8F%96%E5%B7%A8%E6%BD%AE%E8%B5%84%E8%AE%AF%E7%BD%91%E4%BC%81%E4%B8%9A%E4%BF%A1%E6%81%AF/">gongyanli.com</a><br>代码传送门：<a target="_blank" rel="noopener" href="https://github.com/Gladysgong/cninfo">https://github.com/Gladysgong/cninfo</a><br>简书: <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/b5ef0e7e2b87">https://www.jianshu.com/p/b5ef0e7e2b87</a><br>CSDN: <a target="_blank" rel="noopener" href="https://mp.csdn.net/mdeditor/79759833">https://mp.csdn.net/mdeditor/79759833</a></p>
</blockquote>
<pre><code>    首先说说我的目标把，就是抓取巨潮资讯网上一些上市农业企业的基本信息，主要是对页面的公司概况、高管人员、十大股东这几个板块的信
息进行抓取，如图。要抓取的上市农业企业的名单已经准备好了，但是同时要拿到的这些农业企业的url地址。本来考虑的是做一个整站提取url，
但是再想一想，这个网站包含了太多上市公司的信息，即使拿到了，也需要慢慢找。加上我们要抓取的农业企业不多，所以分析页面结果后，手动
整理他们的url，算是本爬虫的一个缺陷。</code></pre>
<p><img src="http://p2lakvkq0.bkt.clouddn.com/cninfo.jpg"></p>
<pre><code>    巨潮资讯地址：http://www.cninfo.com.cn/information/companyinfo_n.html?brief?szmb000998
    上面这个就是公司概况的url地址，而高管人员只需要把brief换成management,十大股东只需要把brief换成shareholders，而后面的后缀
szmb000998这个是手动整理的，这个szmb没看出什么意思，后面的数字是当前公司的股票代码。
    经过分析，发现网页是动态加载的，里面的内容都是通过js来控制iframe进行展现的，通过scrapy中response.body获取网页的返回结果中，
没有完美所需要的内容，所以我们需要用selenium。</code></pre>
<h2 id="一、PhantomJS–PhantomJS安装验证"><a href="#一、PhantomJS–PhantomJS安装验证" class="headerlink" title="一、PhantomJS–PhantomJS安装验证"></a>一、PhantomJS–<a href="http://gongyanli.com/Python3%E7%88%AC%E8%99%AB-PhantomJS%E5%AE%89%E8%A3%85/">PhantomJS安装验证</a></h2><pre><code>PhantomJS是一个基于webkit内核的没有界面的浏览器，所以它和chrome、Firefox这些没有什么差别，只是没有界面而已啦，所以并无高深之处。
关于它的安装及验证非常简单，大家可以参考我的另一篇文章，标题处去点击链接把。</code></pre>
<h2 id="二、Selenium–Selenium的使用"><a href="#二、Selenium–Selenium的使用" class="headerlink" title="二、Selenium–Selenium的使用"></a>二、Selenium–<a target="_blank" rel="noopener" href="https://cuiqingcai.com/5630.html">Selenium的使用</a></h2><pre><code>Selinium是一个自动化的测试工具，用它可以驱动浏览器执行特定的操作，比如点击按钮，切换到ifame中等操作，同时能够获取到浏览器渲染后的
源码。所以它对于那些用JavaScript渲染的网页来讲，Selenium是再合适不过了。
崔庆才的博客中有一篇详细讲解了Selenium的用法，可以参考。</code></pre>
<h2 id="三、通过Scrapy来使用Selenium"><a href="#三、通过Scrapy来使用Selenium" class="headerlink" title="三、通过Scrapy来使用Selenium"></a>三、通过Scrapy来使用Selenium</h2><p><img src="http://p2lakvkq0.bkt.clouddn.com/cninfo1.jpg"></p>
<h3 id="1-中间件"><a href="#1-中间件" class="headerlink" title="1.中间件"></a>1.中间件</h3><pre><code>首先看一下我的工作目录把，没有什么特点，scrapy典型的工作目录，唯一不一样的是middlewares文件夹，里面存放的是我自定义的中间件。通过
自定义的中间件去把scrapy原本的中间件覆盖，从而用我们自己实现的功能去替换scrapy原有的功能。
我的中间件代码如下：打开PhantomJS浏览器，请求url地址，睡眠，接着切换iframe，因为我要获取的公司概况信息就在id=&#39;i_nr&#39;的这个ifame
中，再睡眠，等待浏览器渲染出这个ifame中的内容，然后再body中保存此网页的源码，最后利用HtmlResponse把body传送离开。</code></pre>
<hr>
<pre><code>`from selenium import webdriver
from scrapy.http import HtmlResponse
import time

class JavaScriptMiddleware(object):
    def process_request(self, request, spider):
        if spider.name == &#39;CninfoSpider&#39;:
            print(&#39;PhantomJS1 is starting...&#39;)
            driver = webdriver.PhantomJS(executable_path=r&#39;E:\Program Files\phantomjs-2.1.1-windows\bin\phantomjs.exe&#39;)
            driver.get(request.url)
            time.sleep(1)
            driver.switch_to.frame(&#39;i_nr&#39;)
            time.sleep(2)
            body = driver.page_source
            print(&quot;访问：&quot;, request.url)
            return HtmlResponse(driver.current_url, body=body, encoding=&#39;utf-8&#39;)
        elif spider.name == &#39;CninfoManaSpider&#39;:
            print(&#39;PhantomJS2 is starting...&#39;)
            driver = webdriver.PhantomJS(executable_path=r&#39;E:\Program Files\phantomjs-2.1.1-windows\bin\phantomjs.exe&#39;)
            driver.get(request.url)
            time.sleep(1)
            driver.switch_to.frame(&#39;i_nr&#39;)
            time.sleep(2)
            body = driver.page_source
            print(&quot;访问：&quot;, request.url)
            return HtmlResponse(driver.current_url, body=body, encoding=&#39;utf-8&#39;)
        else:
            return</code></pre>
<p>`</p>
<h3 id="2-数据解析–cninfo-py"><a href="#2-数据解析–cninfo-py" class="headerlink" title="2.数据解析–cninfo.py"></a>2.数据解析–cninfo.py</h3><pre><code>之后，我们就可以回到cninfo.py文件中对内容进行解析了。重写__init__，使用webdriver打开PhantomJS浏览器。重写start_requests方法，
拼接url，同时可以自定义返回函数parse。</code></pre>
<hr>
<pre><code>`# --*-- coding:utf-8 -*-
import scrapy
from scrapy import Request
from scrapy.spiders import Spider
from selenium import webdriver
from ..items import AgriBasicItem

# 巨潮资讯网--上市农业企业基本信息
class CninfoSpider(Spider):
    name = &#39;CninfoSpider&#39;

    def __init__(self):
        self.broswer = webdriver.PhantomJS(
            executable_path=r&#39;E:\Program Files\phantomjs-2.1.1-windows\bin\phantomjs.exe&#39;)
        self.broswer.set_page_load_timeout(30)

    def closed(self, spider):
        print(&#39;spider closed!&#39;)
        self.broswer.close()

    def start_requests(self):
        myurls = [&#39;szmb000998&#39;, &#39;szsme002041&#39;, &#39;szsme002772&#39;, &#39;szcn300087&#39;, &#39;szcn300189&#39;, &#39;szcn300511&#39;,
                       &#39;shmb600108&#39;,
                       &#39;shmb600313&#39;, &#39;shmb600354&#39;, &#39;shmb600359&#39;,
                       &#39;shmb600371&#39;, &#39;shmb600506&#39;, &#39;shmb600598&#39;, &#39;shmb601118&#39;, &#39;szmb000592&#39;, &#39;szsme002200&#39;,
                       &#39;szsme002679&#39;,
                       &#39;shmb600265&#39;, &#39;szmb000735&#39;, &#39;szsme002234&#39;,
                       &#39;szsme002299&#39;, &#39;szsme002321&#39;, &#39;szsme002458&#39;, &#39;szsme002477&#39;, &#39;szsme002505&#39;, &#39;szsme002714&#39;,
                       &#39;szsme002746&#39;, &#39;szcn300106&#39;, &#39;szcn300313&#39;, &#39;szcn300498&#39;,
                       &#39;shmb600965&#39;, &#39;shmb600975&#39;, &#39;szmb000798&#39;, &#39;szsme002086&#39;, &#39;szsme002696&#39;, &#39;szmb200992&#39;,
                       &#39;szcn300094&#39;,
                       &#39;shmb600097&#39;, &#39;shmb600257&#39;, &#39;shmb600467&#39;, &#39;szmb000711&#39;, &#39;szmb000713&#39;]
        start_urls = [
            (&#39;http://www.cninfo.com.cn/information/companyinfo_n.html?brief?&#39; + each) for each in myurls]

        for url in start_urls:
            yield Request(url=url, callback=self.parse)

    def parse(self, response):
        item = AgriBasicItem()

        item[&#39;full_name&#39;] = response.xpath(
            &#39;//div[@class=&quot;clear2&quot;]/div[@class=&quot;zx_left&quot;]/div[2]/table/tbody/tr[1]/td[2]/text()&#39;).extract()[0]  # 公司名称
        item[&#39;en_name&#39;] = response.xpath(
            &#39;//div[@class=&quot;clear2&quot;]/div[@class=&quot;zx_left&quot;]/div[2]/table/tbody/tr[2]/td[2]/text()&#39;).extract()[0]  # 英文名称
        item[&#39;cn_name&#39;] = item[&#39;full_name&#39;]  # 中文名称
        item[&#39;nation&#39;] = &#39;china&#39;  # 国别
        item[&#39;address&#39;] = response.xpath(
            &#39;//div[@class=&quot;clear2&quot;]/div[@class=&quot;zx_left&quot;]/div[2]/table/tbody/tr[3]/td[2]/text()&#39;).extract()[0]  # 注册地址
        item[&#39;established_time&#39;] = None  # 成立时间
        item[&#39;stock_time&#39;] = response.xpath(
            &#39;//div[@class=&quot;clear2&quot;]/div[@class=&quot;zx_left&quot;]/div[2]/table/tbody/tr[13]/td[2]/text()&#39;).extract()[0]  # 上市时间
        # shareholders = scrapy.Field()  # 主要股东
        item[&#39;industry&#39;] = response.xpath(
            &#39;//div[@class=&quot;clear2&quot;]/div[@class=&quot;zx_left&quot;]/div[2]/table/tbody/tr[8]/td[2]/text()&#39;).extract()[
            0]  # 行业(经营类别)
        # managers = scrapy.Field()  # 主要管理人员
        item[&#39;parent_company&#39;] = None  # 母公司
        item[&#39;subsidiaries&#39;] = None  # 子公司
        item[&#39;offical_website&#39;] = response.xpath(
            &#39;//div[@class=&quot;clear2&quot;]/div[@class=&quot;zx_left&quot;]/div[2]/table/tbody/tr[12]/td[2]/text()&#39;).extract()[0]  # 官网
        item[&#39;phone&#39;] = response.xpath(
            &#39;//div[@class=&quot;clear2&quot;]/div[@class=&quot;zx_left&quot;]/div[2]/table/tbody/tr[10]/td[2]/text()&#39;).extract()[0]  # 公司电话
        item[&#39;fax&#39;] = response.xpath(
            &#39;//div[@class=&quot;clear2&quot;]/div[@class=&quot;zx_left&quot;]/div[2]/table/tbody/tr[11]/td[2]/text()&#39;).extract()[0]  # 公司传真
        item[&#39;Twitter&#39;] = None  # Twitter

        item[&#39;stock_code&#39;] = response.xpath(&#39;//div[@class=&quot;zx_info&quot;]/form/table/tbody/tr/td[1]/text()&#39;).extract()[
            0]  # 股票代码
        item[&#39;abbr&#39;] = response.xpath(&#39;//div[@class=&quot;zx_info&quot;]/form/table/tbody/tr/td[1]/text()&#39;).extract()[1]  # 公司简称

        yield item

`</code></pre>
<h3 id="3-数据持久化–pipelines-py"><a href="#3-数据持久化–pipelines-py" class="headerlink" title="3.数据持久化–pipelines.py"></a>3.数据持久化–pipelines.py</h3><pre><code>把抓取下来的数据存进mongo数据库，实现数据的持久化。setting中数据库配置如下：
MONGO_HOST = &#39;127.0.0.1&#39; # 主机ip
MONGO_PORT = 27017    # 端口号
MONGO_DB = &#39;agriEn&#39;     # 数据库名称</code></pre>
<hr>
<pre><code>`import pymongo
from scrapy.conf import settings
from .items import AgriBasicItem
from .items import AgriManaItem


class CninfoPipeline(object):
    def __init__(self):
        self.client = pymongo.MongoClient(host=settings[&#39;MONGO_HOST&#39;], port=settings[&#39;MONGO_PORT&#39;])
        self.db = self.client[settings[&#39;MONGO_DB&#39;]]
        self.cninfo_base = self.db[&#39;cninfo_base&#39;]
        self.cninfo_mana = self.db[&#39;cninfo_mana&#39;]
        self.cninfo_share = self.db[&#39;cninfo_share&#39;]

    def process_item(self, item, spider):
        if isinstance(item, AgriBasicItem):
            try:
                if item[&#39;full_name&#39;]:
                    item = dict(item)
                    self.cninfo_base.insert(item)
                    print(&quot;insert baseinfo success ！&quot;)
                    return item

            except Exception as e:
                spider.logger.exception(&quot;insert failed&quot;)
        elif isinstance(item,AgriManaItem):
            if item[&#39;managers&#39;]:
                item = dict(item)
                self.cninfo_mana.insert(item)
                print(&quot;insert managers success ！&quot;)
                return item`</code></pre>
<h3 id="4-配置文件–setting-py"><a href="#4-配置文件–setting-py" class="headerlink" title="4.配置文件–setting.py"></a>4.配置文件–setting.py</h3><pre><code>ROBOTSTXT_OBEY = False

DOWNLOADER_MIDDLEWARES = &#123;
# 键是中间件类的路径，值是中间的顺序
&#39;cninfo.middlewares.middleware.JavaScriptMiddleware&#39;: 543,
# 禁止内置的中间件
&#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;: None &#125;

# 记得开启pipelines的调用
ITEM_PIPELINES = &#123;
&#39;cninfo.pipelines.CninfoPipeline&#39;: 300,&#125;</code></pre>
<h2 id="四、缺陷"><a href="#四、缺陷" class="headerlink" title="四、缺陷"></a>四、缺陷</h2><pre><code>在这个爬虫中，其实出现了蛮多问题的，有些解决了，有些没有，记录一下，加深印象，还有感觉代码好垃圾。</code></pre>
<h3 id="1-缺陷1"><a href="#1-缺陷1" class="headerlink" title="1.缺陷1"></a>1.缺陷1</h3><pre><code>上市农业企业的名字，以及名字的url链接手动整理的，因为只有40来个企业，所以才会用这个方法。本来是想通过Scarpy的CrawlSpider抓取，
抓取时通过Rule来控制所需要的农业企业，但是发现实在没有什么规律，只会把巨潮资讯上所有上市企业的url都获取下来，所以最后手动整理的
这40多个农业企业，希望日后能找到更简便的办法。</code></pre>
<h3 id="2-缺陷2"><a href="#2-缺陷2" class="headerlink" title="2.缺陷2"></a>2.缺陷2</h3><pre><code>对于网页中的高管人员信息抓取中，本来是想思考放在一个爬虫中获取到的，但是仔细分析后发现，公司概况和高管人员分别对应着不同的url，
然后不同的url下再对应中动态加载iframe。url对比如下：
http://www.cninfo.com.cn/information/companyinfo_n.html?brief?szmb000998
http://www.cninfo.com.cn/information/companyinfo_n.html?management?szmb000998
这样子的话，就涉及到我需要在Selenuim中点击按钮，然后再渲染id=&#39;i_nr&#39;的iframe，而且不同页面中iframe名字都叫i_nr。我试过这个
办法，但是拿回的源码并不是高管人员页面的源码，依然是公司概况页面的源码，也许是我方法用的不对，有待仔细研究。
所以我又在cninfo_mana.py中又写了一个爬虫，从而单独来获取高管人员页面的信息。</code></pre>
<h3 id="3-缺陷3"><a href="#3-缺陷3" class="headerlink" title="3.缺陷3"></a>3.缺陷3</h3><pre><code>对于十大股东页面，按理说我应该在写一个py文件来单独获取它的页面信息，这样子就ok了，实际上我也是这么做的。但是爬虫运行是，却发
现我拿不到股东的信息，因为我发现当请求http://www.cninfo.com.cn/information/companyinfo_n.html?shareholders?
szmb000998时，首先切换到id=&#39;i_nr&#39;的iframe中，但是随后股东的详细信息又在一个id=&#39;i_nr&#39;的ifrma中，相当于这里有两个iframe，
我在中间件试了试两个这样子切换iframe，但是很遗憾我没拿到我想要的东西，源代码还是停留在第一层iframe中，所以最后我放弃了，没有
拿十大股东的信息，这个问题智能留着我以后解决了。</code></pre>
<h3 id="4-缺陷4"><a href="#4-缺陷4" class="headerlink" title="4.缺陷4"></a>4.缺陷4</h3><pre><code>self.broswer = webdriver.PhantomJS(
        executable_path=r&#39;E:\Program Files\phantomjs-2.1.1-windows\bin\phantomjs.exe&#39;)
这个代码在中间件和爬虫代码中都有，说明两次打开了浏览器，致使性能降低。</code></pre>
<h2 id="五、其他项目"><a href="#五、其他项目" class="headerlink" title="五、其他项目"></a>五、其他项目</h2><pre><code>    其实我之前还用Selenium写过一个项目，用过抓取FAO的国家分类信息，以前习惯不好，做事不记录，导致做完就忘，只有个模糊的印象。
今天把之前那个项目找到了，重新运行了一下，但是抓不到东西了，我看看了网站的结构没变，应该是在动态加载方面变化了，等有时间的时候
看看究竟哪里变了，再把代码优化一下，附上github地址，有兴趣可以参考一下。如果有帮助，给个star把，好不要脸，第一次求star。</code></pre>
<p><a target="_blank" rel="noopener" href="https://github.com/Gladysgong/fao">https://github.com/Gladysgong/fao</a></p>
<h2 id="六、后续"><a href="#六、后续" class="headerlink" title="六、后续"></a>六、后续</h2><p>后来我又写了一篇文章——<a href="http://gongyanli.com/%E7%94%A8Python%E4%B8%8B%E8%BD%BD%E5%B7%A8%E6%BD%AE%E8%B5%84%E8%AE%AF%E5%86%9C%E4%B8%9A%E4%B8%8A%E5%B8%82%E4%BC%81%E4%B8%9A%E5%B9%B4%E6%8A%A5/">用Python下载巨潮资讯农业上市企业的年报PDF文件(二)</a>，有兴趣的可以看看。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://gongyanli.com/PhantomJS%E5%AE%89%E8%A3%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lilly">
      <meta itemprop="description" content="Up in the wind!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="茉莉Python">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/PhantomJS%E5%AE%89%E8%A3%85/" class="post-title-link" itemprop="url">PhantomJS安装</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-29 15:40:06" itemprop="dateCreated datePublished" datetime="2018-03-29T15:40:06+08:00">2018-03-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-18 21:39:17" itemprop="dateModified" datetime="2020-09-18T21:39:17+08:00">2020-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/PhantomJS/" itemprop="url" rel="index"><span itemprop="name">PhantomJS</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、下载"><a href="#一、下载" class="headerlink" title="一、下载"></a>一、下载</h2><pre><code>http://phantomjs.org/download.html </code></pre>
<h2 id="二、安装"><a href="#二、安装" class="headerlink" title="二、安装"></a>二、安装</h2><pre><code>解压文件，examlpe中有很多小例子可以试试
把解压后的bin路径配置到path变量中
打开cmd，输入phantomjs验证</code></pre>
<h2 id="三、验证"><a href="#三、验证" class="headerlink" title="三、验证"></a>三、验证</h2><pre><code>`from selenium import webdriver
browser = webdriver.PhantomJS(executable_path=r&#39;E:\Program Files\phantomjs-2.1.1-windows\bin\phantomjs.exe&#39;)
browser.get(&#39;https://www.baidu.com&#39;)
print(browser.current_url)`</code></pre>
<hr>
<pre><code>输出：https:www.baidu.com

成功！</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Lilly</p>
  <div class="site-description" itemprop="description">Up in the wind!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">130</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">45</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lilly</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
